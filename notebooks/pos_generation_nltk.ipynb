{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk_postagger import pos_tag_fn,train_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_h_s = pd.read_csv(\"data_hindi_sanskrit.csv\")\n",
    "df_h_s = pd.read_csv(\"encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s1 = df_h_s[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s1.to_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('csv', data_files=\"MKB_NIOS_data.csv\", header=0, split='train')\n",
    "# dataset = load_dataset('csv',data_files='sample.csv',header = 0, split='train')\n",
    "dataset = load_dataset('csv',data_files=\"encoded.csv\",header=0, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Unnamed: 0' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interlingua_model.model.modelling_mbart_modified_token import MBartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          #DataCollatorForSeq2Seq,\n",
    "                          Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modified data collator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interlingua_model.data_handling.datacollater import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ai4bharat/IndicBART\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =\"/home/ece/utkarsh/interlingua-model/IndicBART/trained-models\")\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(pretrained_model_name_or_path = \"/home/ece/utkarsh/interlingua-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_lang = 'devanagari_kannada' if FLAGS.source_lang.capitalize() == 'Kannada' else FLAGS.source_lang.capitalize()\n",
    "#target_lang = 'devanagari_kannada' if FLAGS.target_lang.capitalize() == 'Kannada' else FLAGS.target_lang.capitalize()\n",
    "source_lang = 'Hindi'\n",
    "target_lang = 'Sanskrit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code = {'Hindi':'hi', 'Sanskrit':'sa', 'Kannada':'kn'}\n",
    "#s_lang = language_code[FLAGS.source_lang.capitalize()]\n",
    "#t_lang = language_code[FLAGS.target_lang.capitalize()]\n",
    "s_lang = 'hi'\n",
    "t_lang = 'sa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "        sentences= examples[source_lang]\n",
    "        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n",
    "        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n",
    "        # print(inputs)\n",
    "        # print(targets)\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "        \n",
    "\n",
    "\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        print(type(labels))\n",
    "        print(type(model_inputs))\n",
    "        print(model_inputs.keys())\n",
    "        print(len(inputs))\n",
    "        lis_pos = []\n",
    "        pos_tagger = train_pos()\n",
    "\n",
    "        for i in tqdm(range(len(inputs))):\n",
    "            # print(len(model_inputs['input_ids'][i]),model_inputs['input_ids'][i])\n",
    "            # print\n",
    "            pos_tag = pos_tag_fn(sentences[i],pos_tagger,tokenizer)\n",
    "            # print(\"len\",len(pos_tag),pos_tag)\n",
    "            if len(model_inputs['input_ids'][i]) != len(pos_tag):\n",
    "                # c+=1\n",
    "                pos_update_val = []\n",
    "                # update_pos = {}\n",
    "                num = len(model_inputs['input_ids'][i]) - 2\n",
    "                # print(i)\n",
    "                pos_update_val += num * [6]\n",
    "                pos_update_val.insert(0,2)\n",
    "                pos_update_val.insert(len(pos_update_val),1)\n",
    "                # print(\"Modified\",len(pos_update_val),pos_update_val)\n",
    "                # update_pos[\"pos\"] = pos_update\n",
    "                lis_pos.append(pos_update_val)\n",
    "            else:\n",
    "                # print('Original',len(pos_tag),pos_tag)\n",
    "                lis_pos.append(pos_tag) \n",
    "        # for i in tqdm(range(len(inputs))):\n",
    "        #     print(i)\n",
    "        #     lis_pos.append(pos_tag_fn(inputs[i],pos_tagger,tokenizer))\n",
    "            #print(i,val)\n",
    "        model_inputs.data['pos'] = lis_pos\n",
    "        # print(model_inputs)\n",
    "        # model_inputs.concat(pos_dict)\n",
    "        \n",
    "        #a.append(model_inputs)\n",
    "        return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Old preprocess function\n",
    "def preprocess_function(examples):\n",
    "        sentences= examples[source_lang]\n",
    "        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n",
    "        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n",
    "        # print(inputs)\n",
    "        # print(targets)\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "        \n",
    "\n",
    "        model_inputs['pos'] = examples['pos']\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        print(type(labels))\n",
    "        print(type(model_inputs))\n",
    "        print(model_inputs.keys())\n",
    "        print(len(inputs))\n",
    "\n",
    "\n",
    "        # print(examples['pos'])\n",
    "        # model_inputs.concat(pos_dict)\n",
    "        \n",
    "        #a.append(model_inputs)\n",
    "        return model_inputs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "#NIos + MKB--12K approx sentence-- 56mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets[\"train\"]['pos'][45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets[\"train\"]['input_ids'][45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets['train']['pos'][45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenized_datasets.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'proc_tokenized_dataset_tensor_nios_mkb.pkl'\n",
    "pickle.dump(tokenized_datasets, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = []\n",
    "rand_sent = []\n",
    "c = 0\n",
    "for i in tokenized_datasets[\"train\"]:\n",
    "    if len(i[\"input_ids\"]) != len(i[\"pos\"]):\n",
    "        # rand_sent.append(i['Hindi'])\n",
    "        # temp = [i['Hindi'],i['pos'],i['input_ids']]\n",
    "        # rand.append(temp)\n",
    "        c +=1\n",
    "        # rand\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('ni-ut')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c0361f45f14327e49d3b6d0d98d289ab7be443aafddbd88be091be9d59a89fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
