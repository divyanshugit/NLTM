{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from NLTM.interlingua_model.model.modelling_mbart_modified_posembedding import MBartForConditionalGeneration\n",
    "from NLTM.interlingua_model.data_handling.datacollater import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"NLTM_POS\", entity=\"divyanshuusingh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          #DataCollatorForSeq2Seq,\n",
    "                          Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ai4bharat/IndicBART\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(pretrained_model_name_or_path = \"/home/ece/utkarsh/interlingua-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seeing class MBartEncoder(MBartPreTrainedModel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model # yeah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model._modules['model']._modules['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._modules['model']._modules['encoder'].embed_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "lets write mbartposembedding---\n",
    "\n",
    "Q1. what input is model taking and then giving to function \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dataset cdac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data = load_from_disk('/home/ece/utkarsh/interlingua-model/IndicBART/data_from_cdac/MKB_NIOS_11k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLTM.interlingua_model.trainer.trainer_seq2seq import Seq2SeqTrainer\n",
    "from NLTM.interlingua_model.trainer import trainer\n",
    "from NLTM.interlingua_model.data_handling.datacollater import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          #DataCollatorForSeq2Seq,\n",
    "                          Seq2SeqTrainingArguments\n",
    "                          )\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "            'translation',\n",
    "            evaluation_strategy='epoch',\n",
    "            learning_rate=0.001,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            weight_decay=0.0001,\n",
    "            #save_total_limit=15,\n",
    "            num_train_epochs=30,\n",
    "            #label_names = [\"pos\"],\n",
    "            predict_with_generate=True,\n",
    "            report_to=\"wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tok_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "        #print(\"compute_met called\")\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        result = {'bleu': result['score']}\n",
    "\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result['gen_len'] = np.mean(prediction_lens)\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        #print(\"compute-met-end\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "              model,\n",
    "              args,\n",
    "              train_dataset=tokenized_datasets['train'],\n",
    "              eval_dataset=tokenized_datasets['test'],\n",
    "              data_collator=data_collator,\n",
    "              tokenizer=tokenizer,\n",
    "              compute_metrics=compute_metrics)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n",
    "\n",
    "predict_dataset = tokenized_datasets['test']\n",
    "predict_results = trainer.predict(predict_dataset,\n",
    "                                    metric_key_prefix='predict',\n",
    "                                    max_length=max_target_length)\n",
    "\n",
    "predictions = tokenizer.batch_decode(predict_results.predictions,\n",
    "                                        skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=True)\n",
    "\n",
    "predictions = [pred.strip() for pred in predictions]\n",
    "\n",
    "# input_sentences = tokenizer.batch_decode(tokenized_datasets['test']['input_ids'], skip_special_tokens=True)\n",
    "# output_sentences = tokenizer.batch_decode(tokenized_datasets['test']['labels'], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'Hindi'\n",
    "target_lang = 'Sanskrit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code = {'Hindi':'hi', 'Sanskrit':'sa', 'Kannada':'kn'}\n",
    "#s_lang = language_code[FLAGS.source_lang.capitalize()]\n",
    "#t_lang = language_code[FLAGS.target_lang.capitalize()]\n",
    "s_lang = 'hi'\n",
    "t_lang = 'sa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if FLAGS.target_lang.capitalize() == 'Kannada':\n",
    "#     kannada_sentences = []\n",
    "#     for sentence in predictions:\n",
    "#         sentence = UnicodeIndicTransliterator.transliterate(sentence, 'hi', 'kn')\n",
    "#         kannada_sentences.append(sentence)      \n",
    "\n",
    "predictions_data = pd.DataFrame({f'{source_lang}':tokenized_datasets['test'][source_lang.capitalize()], \n",
    "                                    f'{target_lang}':tokenized_datasets['test'][target_lang.capitalize()],\n",
    "                                    'predictions':  predictions})\n",
    "\n",
    "predictions_data.to_csv(f'{source_lang}_{target_lang}_predictions.csv')                                                                                                                                                                       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('ni-ut')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c0361f45f14327e49d3b6d0d98d289ab7be443aafddbd88be091be9d59a89fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
