{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic SetUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MKB_NIOS_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files=\"MKB_NIOS_data.csv\", header=0, split='train')\n",
    "# dataset = load_dataset('csv',data_files='sample.csv',header = 0, split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Unnamed: 0' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ai4bharat/IndicBART\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_lang = 'devanagari_kannada' if FLAGS.source_lang.capitalize() == 'Kannada' else FLAGS.source_lang.capitalize()\n",
    "#target_lang = 'devanagari_kannada' if FLAGS.target_lang.capitalize() == 'Kannada' else FLAGS.target_lang.capitalize()\n",
    "source_lang = 'Hindi'\n",
    "target_lang = 'Sanskrit'\n",
    "language_code = {'Hindi':'hi', 'Sanskrit':'sa', 'Kannada':'kn'}\n",
    "#s_lang = language_code[FLAGS.source_lang.capitalize()]\n",
    "#t_lang = language_code[FLAGS.target_lang.capitalize()]\n",
    "s_lang = 'hi'\n",
    "t_lang = 'sa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS SetUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flair_mod.data_mod import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceTagger.load('resources/taggers/final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['<unk>','PSP','NN', 'VM','NNP','SYM','VAUX','JJ','NNPC',\n",
    " 'PRP','CC','NNC','QC','NST','DEM','RP','QF','NEG','RB','QCC',\n",
    " 'QO','INTF','JJC','WQ','RDP','UNK','PRPC','NSTC','RBC','QFC','CCC','INJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le = le.fit(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"आप सब lockdown में इस ‘मन की बात’ को सुन रहे हैं |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence,tokenizer):\n",
    "    # check_sent[0] = sentence\n",
    "    sent_tok= sentence.split(\" \")\n",
    "    sent = Sentence(sentence)\n",
    "    model.predict(sent)\n",
    "    tags = sent.get_labels()\n",
    "    # print(tags)\n",
    "    \n",
    "    pos_tag_val = re.findall(\"'+([A-Z]{2,})'\",str(tags))\n",
    "    encoded_tags = list(le.transform(pos_tag_val))\n",
    "    # encoded_tags.extend([4401,4407])\n",
    "    encoded_tags = [i*3 for i in encoded_tags]\n",
    "    # encoded_tags.insert(0,2)\n",
    "    # encoded_tags.insert(len(encoded_tags),3)\n",
    "    #Test -- Tokinzer\n",
    "    final_encoded_pos = []\n",
    "    token_nlp = tokenizer(sent_tok)\n",
    "    for i in range(len(sent_tok)):\n",
    "        if len(token_nlp.input_ids[i]) == 3:\n",
    "            final_encoded_pos.append(encoded_tags[i])\n",
    "            # try:\n",
    "            #     final_encoded_pos.append(encoded_tags[i])\n",
    "            # except:\n",
    "            #     print(\"Encoded_Tags\",len(encoded_tags),encoded_tags)\n",
    "            #     print(\"Token NLP\", len(token_nlp.input_ids), token_nlp.input_ids)\n",
    "            #     print(\"Sent TOkens\",len(sent_tok),sent_tok)\n",
    "            #     print(sentence)\n",
    "        else:\n",
    "            num_divided_words = len(token_nlp.input_ids[i]) - 2\n",
    "            final_encoded_pos += num_divided_words * [encoded_tags[i]]\n",
    "            # try:\n",
    "            #     final_encoded_pos += num_divided_words * [encoded_tags[i]]\n",
    "            # except:\n",
    "            #     print(\"Encoded_Tags\",len(encoded_tags),encoded_tags)\n",
    "            #     print(\"Token NLP\", len(token_nlp.input_ids), token_nlp.input_ids)\n",
    "            #     print(\"Sent TOkens\",len(sent_tok),sent_tok)\n",
    "            #     print(sentence)\n",
    "\n",
    "    final_encoded_pos.insert(0,2)\n",
    "    final_encoded_pos.extend([4401,4407,3])\n",
    "            \n",
    "            \n",
    "\n",
    "    return final_encoded_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "        sentences= examples[source_lang]\n",
    "        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]\n",
    "        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]\n",
    "        # print(inputs)\n",
    "        # print(targets)\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "        \n",
    "\n",
    "\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        print(type(labels))\n",
    "        print(type(model_inputs))\n",
    "        print(model_inputs.keys())\n",
    "        print(len(inputs))\n",
    "        lis_pos = []\n",
    "        # pos_tagger = train_pos()\n",
    "\n",
    "        for i in tqdm(range(len(inputs))):\n",
    "            # print(len(model_inputs['input_ids'][i]),model_inputs['input_ids'][i])\n",
    "            # print\n",
    "            tag_val = pos_tag(sentences[i],tokenizer)\n",
    "            # print(\"len\",len(tag_val),len(model_inputs['input_ids'][i]))\n",
    "            # print(\"--_--\"*25)\n",
    "            # print('val',sentences[i], tag_val,model_inputs['input_ids'][i])\n",
    "            lis_pos.append(tag_val)\n",
    "            print(\"Checking\",len(tag_val),len(model_inputs['input_ids'][i]))\n",
    "            # if len(model_inputs['input_ids'][i]) != len(pos_tag):\n",
    "            #     # c+=1\n",
    "            #     pos_update_val = []\n",
    "            #     # update_pos = {}\n",
    "            #     num = len(model_inputs['input_ids'][i]) - 2\n",
    "            #     # print(i)\n",
    "            #     pos_update_val += num * [6]\n",
    "            #     pos_update_val.insert(0,2)\n",
    "            #     pos_update_val.insert(len(pos_update_val),1)\n",
    "            #     # print(\"Modified\",len(pos_update_val),pos_update_val)\n",
    "            #     # update_pos[\"pos\"] = pos_update\n",
    "            #     lis_pos.append(pos_update_val)\n",
    "            # else:\n",
    "            #     # print('Original',len(pos_tag),pos_tag)\n",
    "            #     lis_pos.append(pos_tag) \n",
    "        # for i in tqdm(range(len(inputs))):\n",
    "        #     print(i)\n",
    "        #     lis_pos.append(pos_tag_fn(inputs[i],pos_tagger,tokenizer))\n",
    "            #print(i,val)\n",
    "\n",
    "        model_inputs.data['pos'] = lis_pos\n",
    "        # print(model_inputs)\n",
    "        # model_inputs.concat(pos_dict)\n",
    "        \n",
    "        #a.append(model_inputs)\n",
    "        print(\"----Complete----\")\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "#NIos + MKB--12K approx sentence-- 56mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Dataset Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk(\"/nlsasfs/home/ttbhashini/prathosh/divyanshu/pos_hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_dataset = load_from_disk(\"/nlsasfs/home/ttbhashini/prathosh/divyanshu/pos_hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
