1. model_inputs--> {"input_ids":,"attention_mask":,"token_inpus_id":, "pos":}

How model is extracting imput_ids from model_inputs dictionary  (idea---> something happening in seq2seqtrainer(trainer))

Q. why we care--> so we get the pos and create its embedding and add it to inputs_embed (created in encoderlayer of bart....)


2. Improve speed of pos tagger + mapping to tokens [Done]




--------------
collater--returns old: dict_keys(['input_ids', 'attention_mask', 'labels'])  catch: dict is in list
-- returns new dict_keys(['input_ids', 'attention_mask', 'labels', "pos"])

--> next: uncomment pos in model forward--- and create embeddings for pos
---> put pos as "unk" for padded tokens

---> config load-> model load--print model--see if layer present of pos embeddign

Changes made where for pos:
1. line 168 in file trainer_seq2seq.py
2. line 620 in trainer
now pos reaching collater
3. multiple changes in collater.py -- padding etc
4. line 958, 933, 859 in modelling_mbart.py ( adding layer in model)
Now model training but giving error in generation 
5. line 524, 1119 in generation_utils_mod_pos_embedding
6. line 51 modelling_utils
----------------------------training started-----------------------------------------------

1) evaluate-- on test given while train
2) model.bin got---> get bleu score?

PS: 1. input_ids = (input_ids, pos)
    2. model --> encoder layer --> extra layer--> weights loading issue
    3. check args
    4. logger


